{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargad las librerías de numpy, pandas y matplotlib con el nombre de siempre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leed la data  `cleveland_heart_4.csv` cargándolo como un data frame de pandas. Ajustad el argumento na_values='?' y header=None**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clev_data=pd.read_csv('../../datasets/cleveland_heart_4.csv', na_values='?', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Reescribid el nombre de las columnas a:'age', 'sex', 'cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','diagnosis'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_names = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','diagnosis']\n",
    "clev_data.columns = header_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cread una matriz numpy de features llamada `X` y un array numpy de targets llamado `y` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clev_data.drop(columns=['diagnosis']).values\n",
    "y = clev_data['diagnosis'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Como podéis ver, el vector `y` contiene los valores del target, pero éstos están en tipo texto. Scikit solo trabaja con datos numéricos. Por tanto, redefinid `y`, convirtiendo estos targets no numéricos a numéricos usando la clase `LabelEncoder`, que como vimos, se encuentra en el módulo `preprocessing` de scikit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Nuestra dataset contiene variables categóricas, tienen NaNs y además posee muchas features. Por tanto, cread un pipeline que implemento los siguientes pasos: **\n",
    "\n",
    "1. **Imputación de los NaNs de tal forma que sustituya los NaNs en cada columna por la mediana**\n",
    "2. **Ortogonalización de las variables categóricas de tal forma que cada categoría sea una columna nueva. Las variables categóricas son: \"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\" y \"thal\"**\n",
    "3. **Reescalar el rango de los datos a un rango [0,1]**\n",
    "4. **Una feature selection que seleccione los mejores features a partir de un árbol de decisión **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest,f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "oHe = OneHotEncoder(categorical_features=[1,2,5,6,8,10,11,12], sparse=False)\n",
    "ss = MinMaxScaler()\n",
    "#ss = StandardScaler()\n",
    "imp= Imputer(strategy='median')\n",
    "tree_clf = DecisionTreeClassifier(random_state=5)\n",
    "feat_sel = SelectFromModel(tree_clf)\n",
    "#feat_sel = SelectKBest(f_classif, k=10)\n",
    "pip = Pipeline([('imp', imp),('oHe',oHe),('ss',ss),('feat_sel',feat_sel)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comprobad que ha funcionado haciendo un `fit_transform` sobre X e y. Mostrad el output y el tamaño de los datos transformados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pip.fit_transform(X,y))\n",
    "print(pip.fit_transform(X,y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Volved a definir el mismo pipeline de antes, pero incorporando ahora un clasificador a vuestro gusto ( Yo voy a coger SVM) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(class_weight='balanced')\n",
    "pip = Pipeline([('imp', imp),('oHe',oHe),('ss',ss),('feat_sel',feat_sel), ('clf', clf) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cread una partición de los datos y evaluad la performance del modelo usando el pipeline anterior **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)\n",
    "\n",
    "pip.fit(X_train,y_train)\n",
    "print(pip.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Ahora evaluad el modelo mediante la técnia de cross-validation. Podéis elegir el esquema que queráis, por ejemplo, un 10-fold cross-validation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "rcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.mean(cross_val_score(pip, X, y,  cv=rcv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Seguramente, el rendimiento obtenido se pueda mejorar cambiando los hiperparámetros del clasificador y el threshold del feature selection. Mirad esto, sobre la partición X_train, X_test, y_train, y_test de antes, usando la función `GridSearchCV` o `RandomSearchCV` para diferentes valores de los hiperparámetro del clasificador y threshold de la feature selection. Cuidado con el grid que definais y el número de folds que uséis, ya que puede hacer el problema muy time-consuming **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "skf_1 = StratifiedKFold(n_splits=5, random_state=0)\n",
    "\n",
    "param_grid = {'feat_sel__threshold': [0.001,0.01,0.1, None], 'clf__C':[0.01,0.1,1,10]} \n",
    "grid =GridSearchCV(pip, param_grid=param_grid, cv=skf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora, meted esto dentro dentro un cross-validation y calculad la performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf_2 = StratifiedKFold(n_splits=5, random_state=10)\n",
    "\n",
    "print(np.mean(cross_val_score(grid, X, y, cv=skf_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra: Repetid lo mismo de antes con diferentes clasificadores, para que podamos hacernos una idea de cuál es el mejor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin gridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los clasificadores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Hago una lista con los objetos de estos clasificadores y los nombres, sólo para hacer un plot\n",
    "list_clas_names = ['Logistic Regression', 'RandomForest', 'SVM', 'KNN']\n",
    "list_clf = [LogisticRegression(), RandomForestClassifier(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "#Defino el mismo esquema de cross-validation\n",
    "skf_2 = StratifiedKFold(n_splits=5, random_state=10)\n",
    "\n",
    "for i,clf in enumerate(list_clf):\n",
    "    \n",
    "    pip = Pipeline([('imp', imp),('oHe',oHe),('ss',ss),('feat_sel',feat_sel), ('clf', clf)])\n",
    "    \n",
    "    res = cross_val_score(pip, X, y, cv=skf_2)\n",
    "    #print(np.mean(cross_val_score(pip, X, y, cv=skf_2)))\n",
    "    plt.bar(i, np.mean(res), yerr=np.std(res))\n",
    "\n",
    "plt.legend(list_clas_names)\n",
    "plt.ylim([0.7,1.0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con busqueda sobre los hiperparámetros. Hacerlo sobre un grid podría llevar mucho tiempo. Por ello, voy a hacer una búsqueda aleatoria usano `RandomizedSearchCV` con 10 iteraciones por cada clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el módulo de RandomSeachCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Hago una lista de dictionarios para saber los hiperparámetros de cada clasifiador sobre el que buscar\n",
    "list_params = [{'clf__C':[0.01, 0.01,0.1,1,10], 'clf__penalty':['l1', 'l2']},\n",
    "              { 'clf__max_depth':np.arange(1,10), 'clf__criterion':['gini', 'entropy']},\n",
    "              {'clf__C':[0.01,0.1,1,10], 'clf__kernel': [ 'linear', 'poly', 'rbf', 'sigmoid']},\n",
    "              {'clf__n_neighbors':[1,5,10,15, 20], 'clf__weights':['uniform','distance']} ]\n",
    "\n",
    "#Defino el mismo esquema de cross-validation\n",
    "skf_inner = StratifiedKFold(n_splits=5, random_state=0)\n",
    "skf_outer = StratifiedKFold(n_splits=5, random_state=10)\n",
    "\n",
    "for i,clf in enumerate(list_clf):\n",
    "    \n",
    "    pip = Pipeline([('imp', imp),('oHe',oHe),('ss',ss),('feat_sel',feat_sel), ('clf', clf)])\n",
    "    \n",
    "    grid = RandomizedSearchCV(pip, list_params[i], cv=skf_inner, random_state=100)\n",
    "    res = cross_val_score(grid, X, y, cv=skf_outer)\n",
    "    #print(np.mean(cross_val_score(pip, X, y, cv=skf_2)))\n",
    "    plt.bar(i, np.mean(res), yerr=np.std(res))\n",
    "\n",
    "plt.legend(list_clas_names)\n",
    "plt.ylim([0.7,1.0])\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
