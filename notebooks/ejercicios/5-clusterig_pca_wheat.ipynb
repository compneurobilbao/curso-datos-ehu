{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cargad las tres librerías de siempre ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** La data que vamos a usar en este ejercicio corresponde a varias medidas geométricas hechas sobre tres variedades de trigo. La información la podéis encontrar en __[el repositorio UCI](https://archive.ics.uci.edu/ml/datasets/seeds)__ . Cargad y cread un dataframe de pandas a partir del fichero .txt con el nombre `seeds_dataset` que os habéis descargado del repositorio del curso. Este fichero es un txt y no un csv, así  que seguramente tengáis que usar otra función que no sea `read_csv`. Podéis mirar en la documentación de __[pandas](https://pandas.pydata.org/pandas-docs/stable/io.html)__ para saber qué función usar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_seeds = pd.read_table('../../datasets/seeds_dataset.txt',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Mostrad en pantalla info sobre los tipos de las columnas y de si tienen NaNs. Asimismo, mostrad en pantalla información estadística de cada columna, con su medía, quartiles...  ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_seeds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_seeds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Las siete primeras columnas corresponden a las features y la última al target. Cambiad por tanto el nombre de las columnas a ['area', 'perimeter', 'compactness', 'kernel-length', 'kernel-width', 'asymmetry-coefficient', 'kernel-groove-length', 'variety'] **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_seeds.columns= ['area', 'perimeter', \n",
    "  'compactness', 'kernel-length', \n",
    "  'kernel-width', 'asymmetry-coefficient', 'kernel-groove-length', \n",
    "  'variety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_seeds.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cread una matrix numpy `X` con las features y un array `y` con las variedades de trigo **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dat_seeds.drop(columns=['variety']).values\n",
    "y = dat_seeds['variety'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Vamos tratar algún algoritmo de clustering sobre estos datos ( la matriz X de features). Para ello, antes de nada, estandarizar esto datos usando la clase de scikit correspondiente ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_scaled = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Usad Kmeans para ver cómo de buena es la reconstrucción de los clusters. Medid esto usando cualquiera de las métricas vistas en clase. Cuando defináis el objeto de la clase Kmeans, elegid `n_clusters` = 3 (puesto que sabemos que debería haber 3 clases), `random_state` = 0, para que sea reproducible y `n_init` = 100 para asegurarnos encontrar el mejor resultado posible ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, completeness_score, homogeneity_score\n",
    "km = KMeans(n_clusters = 3, random_state=0, n_init=100)\n",
    "km.fit(X_scaled)\n",
    "\n",
    "print(\"adjusted rand score = \" , adjusted_rand_score(y, km.predict(X_scaled)))\n",
    "print(\"adjusted mutual info score = \" , adjusted_mutual_info_score(y, km.predict(X_scaled)))\n",
    "print(\"completeness score = \" , completeness_score(y, km.predict(X_scaled)))\n",
    "print(\"homogeneity score = \" , homogeneity_score(y, km.predict(X_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg = AgglomerativeClustering(n_clusters = 3)\n",
    "\n",
    "y_pred = agg.fit_predict(X_scaled)\n",
    "\n",
    "print(\"adjusted rand score = \" , adjusted_rand_score(y, y_pred))\n",
    "print(\"adjusted mutual info score = \" , adjusted_mutual_info_score(y, y_pred))\n",
    "print(\"completeness score = \" , completeness_score(y, y_pred))\n",
    "print(\"homogeneity score = \" , homogeneity_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gm = GaussianMixture(n_components = 3, n_init=100, random_state=0)\n",
    "\n",
    "gm.fit(X_scaled)\n",
    "\n",
    "y_pred = gm.predict(X_scaled)\n",
    "print(\"adjusted rand score = \" , adjusted_rand_score(y, y_pred))\n",
    "print(\"adjusted mutual info score = \" , adjusted_mutual_info_score(y, y_pred))\n",
    "print(\"completeness score = \" , completeness_score(y, y_pred))\n",
    "print(\"homogeneity score = \" , homogeneity_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Usad el método elbow para mostrar que en efecto el número de clusters óptimo es 3. Acordaos de que el atributo *inertia_* nos daba la información que necesitamos saber y plotear una vez fitteado el kmeans. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for k in np.arange(1,10):\n",
    "\n",
    "    km = KMeans(n_clusters = k, random_state=0)\n",
    "    km.fit(X_scaled)\n",
    "    res.append(km.inertia_)\n",
    "\n",
    "plt.plot( np.arange(1,10), res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X.T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (Opcional) Calculad la matriz de correlaciones entre las features y plotead esta matriz. Para calcular la matriz de correlaciones podeís usar la función `corrcoef` de numpy o usar el método `corr` que tienen los data frames de pandas. Cuidado porque si usáis la función de numpy, la correlación la hace entre las filas y a nosotros nos interesan las columnas, esto es, los features, así que en este caso o bien cuando useis `corrcoef` tenéis que elegir el argumento `rowvar` = False o transponer la matriz antes de llamar a este método. Usad la matriz de features sin estandarizar. Para plotear esta matriz de correlaciones, podéis usar el método `imshow` de matplotlib **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.corrcoef(X, rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dat_seeds.iloc[:,:7].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Realizad una pca sobre la matriz de features (estandarizada, obviamente), de tal manera que nos dé aquellas componentes que expliquen el 95 % de la varianza de los datos. Transformad los datos originales al espacio de las componentes seleccionadas por la PCA **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Haced un scatter plot mostrando los puntos correspondientes a las dos primeras componentes. Acordaos de que para hacer un scatter plot tenéis que usar la función `scatter` de matplotlib. Pasadle como argumento `c` el vector de targets y. ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:,0],X_pca[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:,0],X_pca[:,2], c=dat_seeds.iloc[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:,1],X_pca[:,2], c=dat_seeds.iloc[:,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Mostrad de nuevo el rendimiento del clustering usando Kmeans, pero ahora sobre la matriz de datos transformados por la PCA al espacio de las componentes principales **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 3, random_state=0, n_init=100)\n",
    "km.fit(X_pca)\n",
    "y_pred = km.predict(X_pca)\n",
    "print(\"adjusted rand score = \" , adjusted_rand_score(y, y_pred))\n",
    "print(\"adjusted mutual info score = \" , adjusted_mutual_info_score(y, y_pred))\n",
    "print(\"completeness score = \" , completeness_score(y, y_pred))\n",
    "print(\"homogeneity score = \" , homogeneity_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components = 3, n_init=100, random_state=0)\n",
    "gm.fit(X_pca)\n",
    "y_pred = gm.predict(X_pca)\n",
    "\n",
    "print(\"adjusted rand score = \" , adjusted_rand_score(y, y_pred))\n",
    "print(\"adjusted mutual info score = \" , adjusted_mutual_info_score(y, y_pred))\n",
    "print(\"completeness score = \" , completeness_score(y, y_pred))\n",
    "print(\"homogeneity score = \" , homogeneity_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Uno de los atributos del objeto de la pca cuando lo hemos fitteado es `components_`, que muestra el peso de cada feature en cada componente. Plotead el output de llamar a este atributo usando el metodo `imshow` de matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(pca.components_), cmap = plt.cm.bwr)\n",
    "#plt.imshow(pca.components_)\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
