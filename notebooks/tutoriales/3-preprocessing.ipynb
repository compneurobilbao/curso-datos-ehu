{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DE LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook vamos a ver el preprocessing, en el cual se realizan diferentes transformaciones sobre los datos, bien para eliminar o reemplazar información no útil, o bien para que los algoritmos de clasifiación funcionen correctamente. Por ejemplo, algoritmos como knn, logistic regression y support vector machine necesitan que los datos tengan la misma escala\n",
    "\n",
    "Scikit posee un módulo, `preprocessing`, el cual contiene numerosas herramientas para llevar a cabo dicha operación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('../../datasets/pima-indians-diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando con NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_clean = dataframe.dropna()\n",
    "print(dataframe_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "mat_clean=imp.fit_transform(dataframe.values)\n",
    "print(mat_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando Pandas\n",
    "dataframe.fillna(dataframe.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reescaleando los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mat_clean[:,0:8]\n",
    "y = mat_clean[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerscalear data (Entre 0 and 1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "np.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizar data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar data\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "normalizedX = scaler.fit_transform(X)\n",
    "np.set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarizar los datos\n",
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer = Binarizer(threshold=0.0)\n",
    "binaryX = binarizer.fit_transform(X)\n",
    "np.set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFECTOS DEL PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manejo de diferentes tipos de datos\n",
    "\n",
    "    Hay tres tipos de tipo de datos:\n",
    "        Numericos, e.g. income, age\n",
    "        Categóricos o nominales, e.g. gender, nationality\n",
    "        Ordinales, e.g. low/medium/high\n",
    "\n",
    "    En scikit solo features numéricas\n",
    "\n",
    "    Debemos convertir las variables categóricas y ordinales en numéricas\n",
    "        Create dummy features\n",
    "        Transform a categorical feature into a set of dummy features, each representing a unique category\n",
    "        In the set of dummy features, 1 indicates that the observation belongs to that category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('../../datasets/loan_train.csv')\n",
    "y_train=pd.read_csv('../../datasets/loan_target_train.csv')\n",
    "X_test=pd.read_csv('../../datasets/loan_test.csv')\n",
    "y_test=pd.read_csv('../../datasets/loan_target_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train[['ApplicantIncome', 'CoapplicantIncome','LoanAmount', \n",
    "                   'Loan_Amount_Term', 'Credit_History']],y_train)\n",
    "print(knn.score(X_test[['ApplicantIncome', 'CoapplicantIncome','LoanAmount', \n",
    "                   'Loan_Amount_Term', 'Credit_History']],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['Target'].value_counts()/y_test['Target'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "# Scaling down both train and test data set\n",
    "X_train_minmax=scaler.fit_transform(X_train[['ApplicantIncome', 'CoapplicantIncome',\n",
    "                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n",
    "X_test_minmax=scaler.fit_transform(X_test[['ApplicantIncome', 'CoapplicantIncome',\n",
    "                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train_minmax,y_train)\n",
    "print(knn.score(X_test_minmax,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log= LogisticRegression(penalty='l2',C=.01)\n",
    "log.fit(X_train[['ApplicantIncome', 'CoapplicantIncome','LoanAmount', \n",
    "                   'Loan_Amount_Term', 'Credit_History']], y_train)\n",
    "print(\" Logistic regression antes de preprocessing: \", \n",
    "      log.score(X_test[['ApplicantIncome', 'CoapplicantIncome','LoanAmount', \n",
    "                   'Loan_Amount_Term', 'Credit_History']],y_test))\n",
    "      \n",
    "log.fit(X_train_minmax, y_train)\n",
    "print(\" Logistic regression antes de preprocessing \", log.score(X_test_minmax,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_train_scale=ss.fit_transform(X_train[['ApplicantIncome', 'CoapplicantIncome',\n",
    "                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n",
    "X_test_scale=ss.transform(X_test[['ApplicantIncome', 'CoapplicantIncome',\n",
    "               'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n",
    "# Fitting logistic regression on our standardized data set\n",
    "\n",
    "log.fit(X_train_scale,y_train)\n",
    "print(\"Logistic Regression estandarizando: \", log.score(X_test_scale, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_test.columns:\n",
    "   \n",
    "   if X_test.loc[:,col].dtypes=='object':\n",
    "   \n",
    "        data=X_train.loc[:,col].append(X_test.loc[:,col])\n",
    "        le.fit(data.values)\n",
    "        X_train.loc[:,col]=le.transform(X_train.loc[:,col])\n",
    "        X_test.loc[:,col]=le.transform(X_test.loc[:,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale=ss.fit_transform(X_train)\n",
    "X_test_scale=ss.transform(X_test)\n",
    "\n",
    "log.fit(X_train_scale,y_train)\n",
    "log.score(X_test_scale,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder(sparse=False)\n",
    "X_train_ohe=X_train.copy()\n",
    "X_test_ohe=X_test.copy()\n",
    "columns=['Gender', 'Married', 'Dependents', 'Education','Self_Employed',\n",
    "          'Credit_History', 'Property_Area']\n",
    "for col in columns:\n",
    "    # Creamos una lista de todos los posibles valores categóricos\n",
    "    data=X_train.loc[:,[col]].append(X_test.loc[:,[col]])\n",
    "    enc.fit(data)\n",
    "    # Transformamos los datos usando one hot encoder\n",
    "    temp = enc.transform(X_train.loc[:,[col]])\n",
    "    # Definimos un nuevo data frame\n",
    "    temp=pd.DataFrame(temp,columns=[(col+\"_\"+str(i)) for i in data.loc[:,col]\n",
    "        .value_counts().index])\n",
    "    \n",
    "    temp=temp.set_index(X_train.index.values)\n",
    "    \n",
    "    X_train_ohe=pd.concat([X_train_ohe,temp],axis=1)\n",
    "    \n",
    "    temp = enc.transform(X_test.loc[:,[col]])\n",
    "    \n",
    "    temp=pd.DataFrame(temp,columns=[(col+\"_\"+str(i)) for i in data.loc[:,col]\n",
    "        .value_counts().index])\n",
    "    \n",
    "    temp=temp.set_index(X_test.index.values)\n",
    "    \n",
    "    X_test_ohe=pd.concat([X_test_ohe,temp],axis=1)\n",
    "    X_train_ohe.drop(columns=[col], inplace=True)\n",
    "    X_test_ohe.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale=ss.fit_transform(X_train_ohe)\n",
    "X_test_scale=ss.transform(X_test_ohe)\n",
    "\n",
    "log.fit(X_train_scale,y_train)\n",
    "\n",
    "log.score(X_test_scale,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estos casos, yo recomiendo mejor usar pandas, que te permite hacer el label y one hot enconding fácilmente usando el método `get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('X_train.csv')\n",
    "y_train=pd.read_csv('Y_train.csv')\n",
    "# Importing testing data set\n",
    "X_test=pd.read_csv('X_test.csv')\n",
    "y_test=pd.read_csv('Y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X_train['Married'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df =pd.concat([X_train,X_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Gender', 'Married', 'Dependents', 'Education','Self_Employed',\n",
    "          'Credit_History', 'Property_Area']\n",
    "for col in columns:\n",
    "    dummies = pd.get_dummies(new_df.loc[:,col], prefix=col, dummy_na=False)\n",
    "    new_df = new_df.drop(col, 1)\n",
    "    new_df = pd.concat([new_df, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((X_train_ohe, X_test_ohe)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto como la performance del clasificador puede cambiar según cómo manejemos los datos. No hay forma única y a veces es complicado saber qué procesamiento se debe adoptar. Algunos casos, como Decision Tree y Random Forest, apenas requieren mucho preprocessing. Otros, como support vector machine, logistic regression y knn requieren tratar los datos categóricos y poner todos los datos en la misma escala. Para estos casos, decidir sobre si estandarizar o sólo escalar los datos entre 0 y 1 depende de la naturaleza de los datos en si. Lo mejor, al principio es adoptar las diferentes posibilidades, comparar la performance en cada  y quedarte con el mejor de los casos\n",
    "\n",
    "Referencias:\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
