{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En este notebook vamos a ver las diferentes herramientas que hemos aprendido para evaluar el rendimiento del clasificador y la selección de aquel modelo con el mayor poder predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mam_df = pd.read_csv('../../datasets/mammographic_masses.data', na_values='?', header=None)\n",
    "mam_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mam_df.iloc[:,1:5].values\n",
    "y=mam_df.iloc[:,5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos las transformaciones necesarias sobre nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "oHe = OneHotEncoder(sparse=False, categorical_features=[1,2,3])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_proc=oHe.fit_transform(X) \n",
    "X_proc = scaler.fit_transform(X_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a usar un KNN. Primero, creamos un objeto de la clase de este clasificador\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf= KNeighborsClassifier(weights = 'distance')\n",
    "\n",
    "# Ajustamos los datos\n",
    "clf.fit(X_proc, y)\n",
    "\n",
    "# Predecimos sobre los mismos datos que hemos usado para ajustar\n",
    "print(clf.score(X_proc,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este resultado anterior es irreal y nada generalizable, ya que hemos usado para la predicción el mismo dataset del ajuste. Como hemos visto, para ver la generalización del modelo, tenemos que dejar una parte de la dataset fuera del ajuste (**Método holdout**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_proc=np.concatenate((oHe.fit_transform(X_train), \n",
    "                       scaler.fit_transform(X_train[:,0].reshape(-1,1))), axis=1)\n",
    "X_test_proc=np.concatenate((oHe.transform(X_test), \n",
    "                       scaler.transform(X_test[:,0].reshape(-1,1))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_proc=oHe.fit_transform(X_train) \n",
    "X_train_proc = scaler.fit_transform(X_train_proc)\n",
    "\n",
    "X_test_proc=oHe.transform(X_test) \n",
    "X_test_proc = scaler.transform(X_test_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_proc, y_train)\n",
    "print(\" El rendimiento sobre el training:\" ,clf.score(X_train_proc,y_train))\n",
    "print(\" El rendimiento sobre el test:\", clf.score(X_test_proc,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo dependen los resultados de los parámetros del algoritmo y el tamaño de la partición?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores=[]\n",
    "test_scores=[]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "k_range = np.arange(1,20,1)\n",
    "for neighs in k_range:\n",
    "    \n",
    "    X_train_proc=oHe.fit_transform(X_train) \n",
    "    X_train_proc = scaler.fit_transform(X_train_proc)\n",
    "\n",
    "    X_test_proc=oHe.transform(X_test) \n",
    "    X_test_proc = scaler.transform(X_test_proc)\n",
    "    \n",
    "    clf = KNeighborsClassifier(weights = 'distance',n_neighbors=neighs)\n",
    "\n",
    "    clf.fit(X_train_proc, y_train)\n",
    "\n",
    "    train_scores.append(clf.score(X_train_proc,y_train))\n",
    "    test_scores.append(clf.score(X_test_proc,y_test))\n",
    "    \n",
    "    \n",
    "plt.plot(np.asarray(train_scores))\n",
    "plt.plot(np.asarray(test_scores))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos hacer lo mismo usando la funcionalidad `validation_curve` de scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "train_scores, test_scores = validation_curve(\n",
    "    KNeighborsClassifier(weights = 'distance'), X_proc, y, param_name=\"n_neighbors\", param_range=k_range,\n",
    "    cv=10, scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Validation Curve with KNN\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(k_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(k_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(k_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(k_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.ylim(0.6,1)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores=[]\n",
    "test_scores=[]\n",
    "clf = KNeighborsClassifier(weights = 'distance')\n",
    "\n",
    "sizes=np.linspace(.1, 0.9, 10)\n",
    "\n",
    "for size in sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=size, random_state=0)\n",
    "\n",
    "    X_train_proc=oHe.fit_transform(X_train) \n",
    "    X_train_proc = scaler.fit_transform(X_train_proc)\n",
    "\n",
    "    X_test_proc=oHe.transform(X_test) \n",
    "    X_test_proc = scaler.transform(X_test_proc)\n",
    "        \n",
    "    clf.fit(X_train_proc, y_train)\n",
    "\n",
    "    train_scores.append(clf.score(X_train_proc,y_train))\n",
    "    test_scores.append(clf.score(X_test_proc,y_test))\n",
    "    \n",
    "    \n",
    "plt.plot(sizes, 1-np.asarray(train_scores))\n",
    "plt.plot(sizes, 1-np.asarray(test_scores))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos hacer lo mismo usando la funcionalidad `learning_curve` de scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "train_sizes , train_scores, test_scores = learning_curve(\n",
    "    KNeighborsClassifier(weights = 'distance'), X_proc, y, train_sizes=np.linspace(.1, 1.0, 5),\n",
    "    cv=10, scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a definir un cross-validation con 10 folds estratificados\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "\n",
    "# Veamos cómo es cada fold\n",
    "print('{} {}'.format('Training set observations', 'Testing set observations'))\n",
    "for train_index, test_index in skf.split(X_proc, y):                                     \n",
    "    print('Num obs training: {0}, con {1} de la calse negativa y {2} de la clase positiva. Num obs en test es: {3}'.format(len(train_index), \n",
    "                                                    sum(y[train_index]==0),\n",
    "                                                    sum(y[train_index]==1), \n",
    "                                                    len(test_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf= KNeighborsClassifier(weights = 'distance')\n",
    "scores = cross_val_score(clf, X, y, cv=skf, scoring='accuracy')\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = cross_val_predict(clf, X, y, cv=skf)\n",
    "print(accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos lo mismo que antes, pero de un forma compacta usando la funcionalidad `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = Pipeline([('oHe', oHe),('scaler', scaler),('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pip, X, y, cv=skf, scoring='accuracy')\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1, 20))\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    clf = KNeighborsClassifier(weights = 'distance', n_neighbors=k)\n",
    "    pip = Pipeline([('oHe', oHe),('scaler', scaler),('clf', clf)])\n",
    "    scores = cross_val_score(pip, X, y, cv=skf, scoring='accuracy')\n",
    "    k_scores.append(np.mean(scores))\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras a cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeated cross-validation**\n",
    "\n",
    "- Repetir cross-validation varias veces ( con **differentes particiones aleatorias** de los datos) y promediar los resultados\n",
    "- Estimaciones más fiables puesto que se **reduce la varianza** asociada con un solo intento de cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "rcv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=0)\n",
    "clf = KNeighborsClassifier(weights = 'distance')\n",
    "pip = Pipeline([('oHe', oHe),('scaler', scaler),('clf', clf)])\n",
    "scores = cross_val_score(pip, X, y, cv=rcv, scoring='accuracy')\n",
    "print(len(scores))\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    clf = KNeighborsClassifier(weights = 'distance',n_neighbors=k)\n",
    "    pip = Pipeline([('oHe', oHe),('scaler', scaler),('clf', clf)])\n",
    "    scores = cross_val_score(pip, X, y, cv=rcv, scoring='accuracy')\n",
    "    k_scores.append(np.mean(scores))\n",
    "\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un grid con los parámetros sobre los que probar el clasificador\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un objeto de la clase GridSearchCV\n",
    "grid = GridSearchCV(clf, param_grid, cv=skf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos los datos, que internamente incorpora un cross-validation\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_range, grid.cv_results_['mean_test_score'])\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos mirar las características del mejor modelo\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando varios parámetros simultáneamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos  otra vez los parámetos\n",
    "k_range = list(range(1, 20))\n",
    "weight_options = ['uniform', 'distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora nuestro grid está formado por dos parámetros\n",
    "param_grid = {'clf__n_neighbors':k_range, 'clf__weights':weight_options}\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un objeto de la clase GridSearchCV\n",
    "grid = GridSearchCV(pip, param_grid, cv=skf, scoring='accuracy')\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_).loc[:,['params','mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos mirar las características del mejor modelo\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RandomizedSearchCV` para reducir la carga computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Buscar sobre todas las posibles combinaciones de los diferentes hiperparámetros puede ser muy costoso computacionalmente hablando.\n",
    "- `RandomizedSearchCV` coge un subset de éstos en tantas iteracciones como uno desee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'clf__n_neighbors':k_range, 'clf__weights':weight_options}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_iter controla el número de busquedas sobre los parámetros\n",
    "rand = RandomizedSearchCV(pip, param_dist, cv=10, scoring='accuracy', n_iter=10, random_state=0)\n",
    "rand.fit(X, y)\n",
    "rand.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos mirar las características del mejor modelo\n",
    "print(rand.best_score_)\n",
    "print(rand.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
